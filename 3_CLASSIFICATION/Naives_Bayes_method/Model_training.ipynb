{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a700234",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3102ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"subjectivity\")\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6e61e",
   "metadata": {},
   "source": [
    "## Formal/informal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b160bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "informal_corpus= \"soap-text.txt\"\n",
    "formal_corpus= \"wikipedia-text.txt\"\n",
    "model_name= \"soap_wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8693ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading soap operas text\n",
    "soap_file = open(informal_corpus, \"r\")\n",
    "soap_text = soap_file.read() # string\n",
    "soap_file.close()\n",
    "# tokenizing sentences using punctuations\n",
    "punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "soap_sentences= (punkt_tokenizer.tokenize(soap_text))\n",
    "# tokenizing sentences using spaces (and removal of words containing \"@\")\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "informal_docs= [] # list of tuples, tuples contain tokens and label\n",
    "for sentence in soap_sentences:\n",
    "    tokenized_sentence= space_tokenizer.tokenize(sentence)\n",
    "    if len(tokenized_sentence)>1:\n",
    "        informal_docs.append(([ele for ele in tokenized_sentence if not (re.search('[@#/<>*]', ele))],\"colloquial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0063b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading wikipedia articles text\n",
    "wiki_file = open(formal_corpus, \"r\")\n",
    "wiki_text = wiki_file.read() # string\n",
    "wiki_file.close()\n",
    "# tokenizing sentences using punctuations\n",
    "punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "wiki_sentences= (punkt_tokenizer.tokenize(wiki_text))\n",
    "# tokenizing sentences using spaces (and removal of words containing \"@\")\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "formal_docs= [] # list of tuples, tuples contain tokens and label\n",
    "for sentence in wiki_sentences:\n",
    "    tokenized_sentence= space_tokenizer.tokenize(sentence)\n",
    "    if len(tokenized_sentence)>1: # no single word sentences\n",
    "        formal_docs.append(([ele for ele in tokenized_sentence if not (re.search('[@#/<>*]', ele))],\"formal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd75943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formal, test_formal = nltk.sentiment.util.split_train_test(formal_docs, n=None)\n",
    "train_informal, test_informal = nltk.sentiment.util.split_train_test(informal_docs, n=None)\n",
    "train_docs= train_formal+train_informal\n",
    "test_docs= test_formal+test_informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed4775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words = sentim_analyzer.all_words(train_docs,labeled= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5aabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6efc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ed4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = sentim_analyzer.apply_features(train_docs)\n",
    "test_set = sentim_analyzer.apply_features(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBC= NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "y_pred= []\n",
    "y_test= []\n",
    "for i in tqdm(np.random.randint(0,len(test_set),5000)):\n",
    "    y_pred.append(NBC.classify(test_set[i][0]))\n",
    "    y_test.append(test_docs[i][1])\n",
    "    if y_pred[-1]==y_test[-1]:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "precision= sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad88bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Formal','Colloquial'])\n",
    "ax.yaxis.set_ticklabels(['Formal','Colloquial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01657e",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(\"NBC_\"+model_name+'.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(NBC, file)\n",
    "with open(\"analyzer_\"+model_name+\".pkl\",\"wb\") as file:\n",
    "    pickle.dump(sentim_analyzer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461074d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
