{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3babc88",
   "metadata": {
    "id": "f3babc88"
   },
   "source": [
    "# Method 1: Vector Space Retrieval TF/IDF (simple)\n",
    "\n",
    "Represent both the document and the query by a weight vector in the m-dimensional keyword space assigning non-binary weights and then determine their distance in the m-dimensional keyword space. \n",
    "\n",
    "It is based on a vector space retrieval (VSR) model. For testing the model is used on a simple document collection with about 150 documents in file *blabla.txt*:\n",
    "\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | I think therefore I am\n",
    "  2     | Climate change is a hoax\n",
    "  3     | Fakenews\n",
    "  4     | More beautiful life\n",
    "  5     | Jules and Mr. Gilleron are very friendly\n",
    "  6     | Road to Minor improvement\n",
    "  7     | In love with the Rolex meeting rooms\n",
    "  8     | The important thing is to laugh\n",
    "  9     | A small step for man but a big step for humanity\n",
    " ...    | ...\n",
    " 160     | The music is too loud for my ears.\n",
    " \n",
    " \n",
    " In information retrieval, tf–idf, TF\\*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. For more info: [link](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    " \n",
    " Properties: \n",
    " - Ranking of documents according to similarity value\n",
    " - Documents can be retrieved even if they don't contain some query keyword\n",
    " - fast to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7a554f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db7a554f",
    "outputId": "a1d2b7a0-4ac2-471e-d691-e4f906a9a761"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary in the documents\n",
      " ------------------------------\n",
      " ['100', '20000', '7am', 'a', 'aaron', 'abl', 'accept', 'address', 'ador', 'afternoon', 'ahead', 'air', 'all', 'amaz', 'ancestor', 'angel', 'anoth', 'answer', 'anyth', 'apolog', 'appreci', 'are', 'around', 'ask', 'ate', 'avail', 'away', 'bad', 'bag', 'basebal', 'bean', 'beauti', 'best', 'big', 'bigger', 'birthday', 'black', 'blind', 'block', 'blow', 'board', 'bodi', 'book', 'boot', 'bore', 'boston', 'bring', 'brother', 'brush', 'bu', 'bueno', 'call', 'can', 'candl', 'car', 'care', 'carelessli', 'cash', 'cat', 'catch', 'chang', 'checkin', 'children', 'chime', 'circl', 'class', 'classgroup', 'climat', 'clock', 'close', 'closest', 'coffe', 'color', 'come', 'commit', 'committe', 'common', 'commut', 'corn', 'corner', 'correct', 'could', 'cri', 'cross', 'crossroad', 'crowd', 'danc', 'day', 'dead', 'delici', 'depress', 'desk', 'did', 'dinner', 'direct', 'disappoint', 'discount', 'dish', 'dizzi', 'do', 'doe', 'dog', 'dont', 'door', 'dou', 'drag', 'dri', 'drink', 'drive', 'drove', 'ear', 'earach', 'eat', 'echo', 'elizabeth', 'els', 'emili', 'english', 'enjoy', 'everi', 'everyon', 'everyth', 'exam', 'excus', 'exhaust', 'expens', 'explain', 'extend', 'fabul', 'fakenew', 'famili', 'fantast', 'far', 'favorit', 'feel', 'find…', 'fine', 'finger', 'first', 'fit', 'five', 'fli', 'floor', 'florida', 'food', 'forc', 'four', 'fresh', 'friday', 'friend', 'friendli', 'frog', 'front', 'fun', 'get', 'gift', 'gilleron', 'give', 'glutenfre', 'go', 'gobbl', 'godspe', 'good', 'grace', 'grainfre', 'great', 'green', 'greet', 'groceri', 'grow', 'guaranteewarranti', 'guest', 'ham', 'handbag', 'handknit', 'happi', 'have', 'haw', 'he', 'head', 'hear', 'heart', 'heel', 'help', 'hem', 'high', 'hoax', 'homemad', 'homework', 'hope', 'hotel', 'hour', 'how', 'human', 'hurriedli', 'i', 'idea', 'idl', 'im', 'import', 'improv', 'in', 'incess', 'infatu', 'inspir', 'into', 'invit', 'ira', 'is', 'it', 'itali', 'item', 'jacket', 'jacob', 'jame', 'jar', 'jean', 'jenni', 'jessi', 'joan', 'joe', 'join', 'joy', 'juarez', 'juic', 'jule', 'jump', 'june', 'kelli', 'key', 'knock', 'know', 'land', 'larg', 'last', 'late', 'laugh', 'leather', 'left', 'less', 'letter', 'librari', 'life', 'lifetim', 'light', 'like', 'live', 'lo', 'london', 'longdead', 'longshort', 'look', 'lost', 'lot', 'loud', 'louder', 'love', 'luck', 'made', 'make', 'man', 'mangi', 'mani', 'map', 'marri', 'mash', 'math', 'may', 'mayb', 'meet', 'meter', 'miami', 'middl', 'mind', 'minor', 'misha', 'miss', 'mitten', 'money', 'more', 'morn', 'mother', 'move', 'movi', 'mr', 'much', 'music', 'my', 'name', 'near', 'nearest', 'need', 'nervous', 'next', 'nice', 'night', 'noisi', 'not', 'occupi', 'offer', 'offic', 'old', 'on', 'one', 'open', 'organ', 'pack', 'page', 'pair', 'pant', 'paper', 'pardon', 'parent', 'park', 'parti', 'pass', 'pencil', 'peopl', 'philippin', 'pictur', 'pizza', 'plan', 'play', 'pleas', 'polici', 'pond', 'post', 'potato', 'pricey', 'printer', 'public', 'pull', 'pulldown', 'put', 'question', 'quickli', 'rather', 'read', 'readi', 'realli', 'reason', 'red', 'refriger', 'refund', 'repair', 'repeat', 'reserv', 'retir', 'return', 'right', 'rins', 'rip', 'road', 'rolex', 'room', 'run', 'sad', 'safe', 'samantha', 'sarah', 'sat', 'saturday', 'say', 'scarf', 'school', 'scrawni', 'second', 'see', 'sell', 'sens', 'sentenc', 'she', 'shop', 'show', 'shower', 'sibl', 'sick', 'sign', 'similar', 'simpl', 'singl', 'size', 'sleepi', 'small', 'smaller', 'smell', 'smith', 'snow', 'sock', 'someon', 'someth', 'sorri', 'sound', 'speak', 'special', 'spoke', 'stain', 'start', 'station', 'stay', 'step', 'stock', 'stomachach', 'stood', 'stop', 'store', 'straight', 'strawberri', 'stray', 'street', 'stretch', 'stripe', 'suitcas', 'summer', 'sunday', 'sure', 'sweater', 'swim', 'switch', 'süpermarket', 'take', 'talk', 'teacher', 'teeth', 'tell', 'thank', 'that', 'the', 'there', 'therefor', 'they', 'thi', 'thing', 'think', 'thought', 'three', 'threw', 'thursday', 'tick', 'time', 'tipto', 'tomorrow', 'top', 'towel', 'to…', 'transport', 'travel', 'tri', 'tube', 'tuesday', 'turn', 'tv', 'twirl', 'unabl', 'unfold', 'up…', 'us', 'use', 'vacat', 'veget', 'wake', 'wakeup', 'walk', 'wall', 'want', 'watch', 'waterproof', 'way', 'we', 'weather', 'week', 'weight', 'went', 'what', 'whe', 'when', 'where', 'which', 'will', 'window', 'winter', 'wish', 'wishin', 'without', 'wonder', 'work', 'would', 'wrinkl', 'write', 'ye', 'year', 'yet', 'you', 'your', '’', '…']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"blabla.txt\", encoding='utf8') as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()\n",
    "print('Vocabulary in the documents\\n ------------------------------\\n', vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950064c4",
   "metadata": {
    "id": "950064c4"
   },
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6829d",
   "metadata": {
    "id": "6ff6829d"
   },
   "source": [
    "Now, for the query $Q$ = $love$, here the top ranked documents according to the TF/IDF rank using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36c4efe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a36c4efe",
    "outputId": "b4636eff-bb17-4e94-f1c3-73cd3ef30170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correspondances: 3\n",
      "----------------------------\n",
      "Watching TV and movies is something that most people enjoy\n",
      "I will meet with my friends on Friday.\n",
      "Jules and Mr. Gilleron are very friendly\n"
     ]
    }
   ],
   "source": [
    "ret_ids = search_vec_sklearn('Jules loves watching TV with friends', features)\n",
    "print('Found correspondances:', len(ret_ids))\n",
    "print('----------------------------')\n",
    "for i, v in enumerate(ret_ids):\n",
    "    print(original_documents[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2bc9f",
   "metadata": {
    "id": "70e2bc9f"
   },
   "source": [
    "# Method 2: Vector space retrieval TF/IDF + Query expension\n",
    "\n",
    "Implementation and test of [Rocchio's method](https://en.wikipedia.org/wiki/Rocchio_algorithm) for user relevance feedback. Like many other retrieval systems, the Rocchio feedback approach was developed using the Vector Space Model. The algorithm is based on the assumption that most users have a general conception of which documents should be denoted as relevant or non-relevant. Therefore, the user's search query is revised to include an arbitrary percentage of relevant and non-relevant documents as a means of increasing the search engine's recall, and possibly the precision as well. The number of relevant and non-relevant documents allowed to enter a query is dictated by the weights of the a, b, c variables listed below in the Algorithm section.\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_j} \\in D_{nr}} \\vec{d_j}\n",
    "\\end{equation}\n",
    "In the Rocchio algorithm negative term weights are ignored. This means, for the negative term weights in $\\vec{q_m}$, we set them to be 0.\n",
    "\n",
    "Properties:\n",
    "\n",
    "- System adds query terms to user query\n",
    "- Use of the top $k$ documents to enriche the initial querry\n",
    "- Modified queries are complex $\\rightarrow$ expensive processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d388488",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d388488",
    "outputId": "8ca9a77e-c7b4-4e6e-d01f-436dcde920cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "#with open(\"bread.txt\") as f:\n",
    "with open(\"blabla.txt\", encoding = 'utf8') as f:\n",
    "    content = f.readlines()\n",
    "#original_documents = [x.decode('utf-8').strip() for x in content] # for python2\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "F9vtCPsUAFz1",
   "metadata": {
    "id": "F9vtCPsUAFz1"
   },
   "outputs": [],
   "source": [
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd39f43",
   "metadata": {
    "id": "bcd39f43"
   },
   "source": [
    "Shaping the modified vector in a direction closer, or farther away, from the original query, related documents, and non-related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ab1077",
   "metadata": {
    "id": "67ab1077"
   },
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "\n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = query_vector*alpha\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = [beta*sum(x)/num_rel for x in zip(*relevant_doc_vecs)]\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = [-gamma*sum(x)/num_non_rel for x in zip(*non_relevant_doc_vecs)]\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = [sum(x) for x in zip(norm_sum_relevant, norm_sum_non_relevant, norm_query_vector)]\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = [x if x>0 else 0 for x in modified_query_vector]\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce6de29d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce6de29d",
    "outputId": "055a18c7-efe5-494f-b594-4200657859cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I love being around you.\n",
      "1 Jules and Mr. Gilleron are very friendly\n",
      "2 In love with the Rolex meeting rooms\n",
      "3 Emily is similar to James. They both love to dance.\n",
      "4 I think therefore I am\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"Jules loves paragliding\", 5)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SuTawJcpBTWd",
   "metadata": {
    "id": "SuTawJcpBTWd"
   },
   "source": [
    "If we fix that the first 3 elements resulting from the initial query are relevant to enrich the querry then we have the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39bb6a54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39bb6a54",
    "outputId": "4388ed04-d11e-4ea9-8a43-988fc5e84c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  around friendli gilleron jule love meet mr rolex room veri\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jules and Mr. Gilleron are very friendly',\n",
       " 'In love with the Rolex meeting rooms',\n",
       " 'I love being around you.',\n",
       " 'Are you from around here?',\n",
       " 'Mrs. Juarez and Mr. Smith are dancing gracefully.',\n",
       " 'It is nice to meet you',\n",
       " 'I will meet with my friends on Friday.',\n",
       " 'Misha walked and looked around.',\n",
       " 'Where is the changing room?',\n",
       " 'The room is too noisy.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "# suppose first three documents were relevant and the rest were irrelevant.\n",
    "relevant_indices = [0,1,2]\n",
    "non_relevant_indices = [i for i in range(3, len(ans))]\n",
    "\n",
    "relevant_doc_ids = [result_doc_ids[i] for i in relevant_indices]\n",
    "non_relevant_doc_ids = [result_doc_ids[i] for i in non_relevant_indices]\n",
    "\n",
    "relevant_doc_vecs = [document_vectors[i] for i in relevant_doc_ids]\n",
    "non_relevant_doc_vecs = [document_vectors[i] for i in non_relevant_doc_ids]\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , not_important_now, idontcare_anymore = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07319e",
   "metadata": {
    "id": "9e07319e"
   },
   "source": [
    "# Method 3: Word embeddings\n",
    "In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "For this method, we would train word embeddings using a state-of-the-art embeddings [library fastText](https://ai.facebook.com/tools/fasttext) (facebook).\n",
    "\n",
    "Note that it is only available on linux and iOS :( $\\rightarrow$ Virtual Machine or Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900a4136",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "900a4136",
    "outputId": "a22f5680-180b-4c01-a558-997d4909f375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\alexb\\mambaforge\\lib\\site-packages (from fasttext) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\alexb\\mambaforge\\lib\\site-packages (from fasttext) (57.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexb\\mambaforge\\lib\\site-packages (from fasttext) (1.21.2)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for fasttext\n",
      "Failed to build fasttext\n",
      "Installing collected packages: fasttext\n",
      "    Running setup.py install for fasttext: started\n",
      "    Running setup.py install for fasttext: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\alexb\\mambaforge\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\alexb\\AppData\\Local\\Temp\\pip-wheel-_gswk_vs'\n",
      "       cwd: C:\\Users\\alexb\\AppData\\Local\\Temp\\pip-install-fntns2hh\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\n",
      "  Complete output (20 lines):\n",
      "  C:\\Users\\alexb\\mambaforge\\lib\\site-packages\\setuptools\\dist.py:697: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-3.9\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\n",
      "  creating build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "  creating build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "  running build_ext\n",
      "  building 'fasttext_pybind' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\alexb\\mambaforge\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\alexb\\AppData\\Local\\Temp\\pip-record-urfn6zla\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\alexb\\mambaforge\\Include\\fasttext'\n",
      "         cwd: C:\\Users\\alexb\\AppData\\Local\\Temp\\pip-install-fntns2hh\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\n",
      "    Complete output (20 lines):\n",
      "    C:\\Users\\alexb\\mambaforge\\lib\\site-packages\\setuptools\\dist.py:697: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "      warnings.warn(\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\fasttext\n",
      "    copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-3.9\\fasttext\n",
      "    copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\n",
      "    creating build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "    copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "    copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\\util\n",
      "    creating build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-3.9\\fasttext\\tests\n",
      "    running build_ext\n",
      "    building 'fasttext_pybind' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\alexb\\mambaforge\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\alexb\\\\AppData\\\\Local\\\\Temp\\\\pip-install-fntns2hh\\\\fasttext_e73ccb23735b4578ae1fc6fdb7c2e2b4\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\alexb\\AppData\\Local\\Temp\\pip-record-urfn6zla\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\alexb\\mambaforge\\Include\\fasttext' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "#if necessary \n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ScQwSp4mec",
   "metadata": {
    "id": "d7ScQwSp4mec"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec5fc61",
   "metadata": {
    "id": "0ec5fc61"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fasttext' has no attribute 'train_unsupervised'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30376/280622209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'blabla.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cbow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'fasttext' has no attribute 'train_unsupervised'"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised('blabla.txt', model = 'cbow')\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aeb522",
   "metadata": {
    "id": "f2aeb522"
   },
   "source": [
    "\n",
    "### Visualization of the Embeddings\n",
    "\n",
    "Visualization the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities). t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57db60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd57db60",
    "outputId": "d05517ed-f04d-470e-ce1b-e9161c33d06b"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') \n",
    "vis_data = tsne.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ec7c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d09ec7c6",
    "outputId": "9b6ca999-3608-4941-a253-062d2c3d1274"
   },
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79001b0",
   "metadata": {
    "id": "094ca938"
   },
   "source": [
    "### Basic Search Engine Using Word Embeddings\n",
    "In this part, we would put our word embeddings to test by using them for information retrieval. The idea is that, the documents that have the most similar embedding vectors to the one belongs to query should rank higher. The documents may not necessarily include the keywords in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"blabla.txt\", encoding = 'utf8') as f:\n",
    "    content = f.readlines()\n",
    "        \n",
    "original_documents = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99deb48c",
   "metadata": {},
   "source": [
    "Since both the documents and the query is of variable size, we should aggregate the vectors of the words in the query by some strategy. This could be taking the minimum vector, maximum vector or the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "for aggfunc in possible_aggfuncs:\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(original_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in enumerate(original_documents):\n",
    "        vlist = [vector_dict[token] for token in fasttext.tokenize(doc) if token in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d473168",
   "metadata": {},
   "source": [
    "Then we have to aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"EPFL\"\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    tokens = fasttext.tokenize(query)\n",
    "    if(len(tokens) == 1):\n",
    "        if(tokens[0] in vocabulary):\n",
    "            return vector_dict[tokens[0]]\n",
    "    elif(len(tokens) > 1):\n",
    "        vlist = []\n",
    "        for token in tokens:\n",
    "            if (token in vocabulary):\n",
    "                vlist.append(vector_dict[token])\n",
    "        \n",
    "        return aggregate_vector_list(vlist, aggfunc)\n",
    "    else:\n",
    "        print(\"%s is not in the vocabulary.\" % (query))\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    # Calculate the similarity with each vector. \n",
    "    #Cosine similarity function takes a matrix as input so we do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector, aggregated_doc_vectors[aggfunc])\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with \n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    for index in indexes:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c224c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "methods_language_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
