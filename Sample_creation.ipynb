{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e0ae53-d531-4f7a-a960-bc96f6812343",
   "metadata": {},
   "source": [
    "### Sample creation\n",
    "Here, we create a sample, which will contains around 200,00 quotes per year, which will be usefull for testing our different programms on it instead of running them on the full corpus. The whole corpus will be opened by chunks and we'll take random rows, uniformely for each year, in order to get the desired amount of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e499bac-08f5-4413-899b-d1abac1afa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db45778-4d4c-4cc7-af3e-a4c2a2401e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of years available and used\n",
    "years = [2015,2016,2017,2018,2019,2020]\n",
    "\n",
    "# Size of years file in GB\n",
    "year_size = np.array([3.3,2.3,5.2,4.8,3.6,.9])\n",
    "\n",
    "# Number of rows in file of 2020 (computed once \"by hand\", as long as this file is not too heavy)\n",
    "rows2020 = 5244449\n",
    "\n",
    "# Estimation of number of rows in each file\n",
    "year_rows = year_size*rows2020/.9\n",
    "\n",
    "# Which correspond to n chunks of size one million\n",
    "chunks_number = np.rint(year_rows/1e6).astype(int)\n",
    "\n",
    "# Which means we have to take n random rows in each chunk to get a sample of 200,000 rows per year\n",
    "rows_in_chunk = np.rint(2*1e5/chunks_number).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551a97a0-015d-409b-9918-b607078e6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As long as year 2017 crashes (even run alone), we change parameters for it\n",
    "rows_in_chunk[2] = 666\n",
    "chunks_number[2] = 300\n",
    "\n",
    "# And we chose different chunk size for this year\n",
    "chunk_sizes = [1e6,1e6,1e5,1e6,1e6,1e6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627fb2b0-8c9b-4ee8-89fb-5179c4efd069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10526, 15385,   666,  7143,  9524, 40000]),\n",
       " array([ 19,  13, 300,  28,  21,   5]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_in_chunk, chunks_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2441896a-ca88-4142-9e48-9d5e5604d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files all have the following columns\n",
    "Index = ['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1d403e-ae4d-4b9d-bd7d-cdc89467a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function, when testing loops\n",
    "def process_chunk(chunk,year):\n",
    "        print(f'Processing chunk with {len(chunk)} rows, in file from year {year}')\n",
    "        # print(chunk.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c2fab1d-8830-45c7-994c-fe32edb1a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est parti, mon kiki !\n",
      "\n",
      "Start year 2015 with chunks of size 1000000\n",
      "Done with year 2015\n",
      "Start year 2016 with chunks of size 1000000\n",
      "Done with year 2016\n",
      "Start year 2017 with chunks of size 100000\n",
      "Done with year 2017\n",
      "Start year 2018 with chunks of size 1000000\n",
      "Done with year 2018\n",
      "Start year 2019 with chunks of size 1000000\n",
      "Done with year 2019\n",
      "Start year 2020 with chunks of size 1000000\n",
      "Done with year 2020\n",
      "\n",
      "Tout est bien qui finit bien\n"
     ]
    }
   ],
   "source": [
    "print(\"C'est parti, mon kiki !\\n\")\n",
    "df = pd.DataFrame(columns=Index)\n",
    "for iii, year in enumerate(years) :\n",
    "    with pd.read_json(f'./Quotebank/quotes-{year}.json.bz2', lines=True, compression='bz2', chunksize=chunk_sizes[iii]) as df_reader:\n",
    "        print(f\"Start year {year} with chunks of size {int(chunk_sizes[iii])}\")\n",
    "        for chunk in df_reader:\n",
    "            # process_chunk(chunk,year)\n",
    "            df = pd.concat([df,chunk.sample(rows_in_chunk[iii])])\n",
    "        print(f\"Done with year {year}\")\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_json(f\"./Quotebank/Sample.json.bz2\",compression=\"bz2\",lines=True,orient=\"records\")\n",
    "print(\"\\nTout est bien qui finit bien\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
