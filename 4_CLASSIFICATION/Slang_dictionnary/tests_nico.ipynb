{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9babeebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicola\\AppData\\Local\\Temp/ipykernel_9580/446153002.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746bf20e224c4e359a0c38ade95f28c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#packages\n",
    "#to install in our environment : conda install -c conda-forge ipywidgets jupyter nbextension enable --py widgetsnbextension\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95a56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check if strin is contained and update dico\n",
    "def isinside1(test_string,test_list):\n",
    "    global dico\n",
    "    res = [ele for ele in test_list if(ele in test_string)]\n",
    "    #print(res)\n",
    "    if res:\n",
    "        for ele in res:\n",
    "            i= test_list.index(ele)\n",
    "            dico[\"occurences\"].loc[i]= dico[\"occurences\"].loc[i]+1\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isinside2(test_string,test_list):\n",
    "    if any(ext in test_string for ext in test_list):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c50ab2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDid</th>\n",
       "      <th>word</th>\n",
       "      <th>example</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Janky</td>\n",
       "      <td>Your mother cooks janky collard greens.</td>\n",
       "      <td>310</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>slumpin'</td>\n",
       "      <td>slumpin' beats and phat tracks as we spin da w...</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>yayeeyay</td>\n",
       "      <td>\"We be dubbin!  And we's gonna bounce to play ...</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>hard-core</td>\n",
       "      <td>Dood, that girl's pants are too hard-core for us.</td>\n",
       "      <td>167</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>brutal</td>\n",
       "      <td>Man, this morning's calisthenics were brutal.</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDid       word                                            example  \\\n",
       "0     7      Janky            Your mother cooks janky collard greens.   \n",
       "1     8   slumpin'  slumpin' beats and phat tracks as we spin da w...   \n",
       "2     9   yayeeyay  \"We be dubbin!  And we's gonna bounce to play ...   \n",
       "3    12  hard-core  Dood, that girl's pants are too hard-core for us.   \n",
       "4    13     brutal      Man, this morning's calisthenics were brutal.   \n",
       "\n",
       "   upvotes  downvotes  occurences  \n",
       "0      310        266           0  \n",
       "1       19         39           0  \n",
       "2       23         28           0  \n",
       "3      167         96           0  \n",
       "4       12         45           0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get dico\n",
    "dico= pd.read_pickle(\"./new_urban_dictionary.pkl\")\n",
    "dico[\"occurences\"]=0\n",
    "dico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e5cb7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDid</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>occurences</th>\n",
       "      <th>strlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16101.000000</td>\n",
       "      <td>16101.000000</td>\n",
       "      <td>16101.000000</td>\n",
       "      <td>16101.0</td>\n",
       "      <td>16101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24104.422210</td>\n",
       "      <td>203.544873</td>\n",
       "      <td>101.174958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.315695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14168.359776</td>\n",
       "      <td>1240.494452</td>\n",
       "      <td>518.662357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.530259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11987.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24048.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36149.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49993.000000</td>\n",
       "      <td>81480.000000</td>\n",
       "      <td>29669.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UDid       upvotes     downvotes  occurences        strlen\n",
       "count  16101.000000  16101.000000  16101.000000     16101.0  16101.000000\n",
       "mean   24104.422210    203.544873    101.174958         0.0      8.315695\n",
       "std    14168.359776   1240.494452    518.662357         0.0      4.530259\n",
       "min        7.000000      0.000000      0.000000         0.0      1.000000\n",
       "25%    11987.000000      6.000000      8.000000         0.0      5.000000\n",
       "50%    24048.000000     18.000000     23.000000         0.0      7.000000\n",
       "75%    36149.000000     74.000000     64.000000         0.0     10.000000\n",
       "max    49993.000000  81480.000000  29669.000000         0.0    117.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check strings lenths and describe dico\n",
    "dico[\"strlen\"]= dico[\"word\"].apply(lambda x : len(x))\n",
    "dico.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3506e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pussy',\n",
       " 'orgasm',\n",
       " 'Chode',\n",
       " 'cunt',\n",
       " 'cum',\n",
       " \"fo' shizzle my nizzle\",\n",
       " 'stfu',\n",
       " 'fap',\n",
       " 'Rusty Trombone',\n",
       " 'Procrasturbating',\n",
       " 'pussy',\n",
       " 'bukkake',\n",
       " 'productive procrastination',\n",
       " 'butter face',\n",
       " 'asshat',\n",
       " 'virgin',\n",
       " 'Masturbation',\n",
       " 'rap',\n",
       " 'Spooning',\n",
       " 'gay buffer',\n",
       " 'Whore',\n",
       " 'pink sock',\n",
       " 'camel toe',\n",
       " 'hoe',\n",
       " 'Fugly',\n",
       " 'punk',\n",
       " 'DTF',\n",
       " 'shawty',\n",
       " 'You',\n",
       " 'salty',\n",
       " 'docking',\n",
       " 'ghetto',\n",
       " 'shocker',\n",
       " 'weed',\n",
       " 'eiffel tower',\n",
       " 'FFS',\n",
       " 'porn storm',\n",
       " 'deez nuts',\n",
       " 'Butterface',\n",
       " 'PETA',\n",
       " 'metallica',\n",
       " 'boobies',\n",
       " 'Word up, kids?',\n",
       " 'Boobs',\n",
       " 'pimp',\n",
       " 'wtf',\n",
       " 'ding dong ditch',\n",
       " 'boner',\n",
       " 'redneck',\n",
       " 'blunt',\n",
       " 'baller',\n",
       " 'masturbation',\n",
       " 'crack',\n",
       " 'lawl',\n",
       " 'KKK',\n",
       " 'blink 182',\n",
       " 'Jack',\n",
       " 'automagically',\n",
       " 'w00t',\n",
       " 'shoot the shit',\n",
       " 'Glomp',\n",
       " 'jake',\n",
       " 'masturbate',\n",
       " 'nfi',\n",
       " 'IMO',\n",
       " 'porn',\n",
       " 'nirvana',\n",
       " 'whore',\n",
       " 'bia',\n",
       " 'snarky',\n",
       " 'Second Base',\n",
       " 'xxx',\n",
       " 'Happy ending',\n",
       " 'nonce',\n",
       " 'SHIT LOCKS',\n",
       " 'dp',\n",
       " 'emo',\n",
       " 'racist',\n",
       " 'cooter',\n",
       " 'spinner',\n",
       " 'skeezy',\n",
       " 'smegma',\n",
       " 'fly',\n",
       " 'pikachu',\n",
       " 'wumbo',\n",
       " 'Randy',\n",
       " 'afk',\n",
       " 'No u',\n",
       " 'Gunt',\n",
       " 'homie',\n",
       " 'iirc',\n",
       " 'Banana',\n",
       " 'snatch',\n",
       " 'ttyl',\n",
       " 'white people',\n",
       " 'shizzle',\n",
       " 'babe',\n",
       " 'anal',\n",
       " 'fluffer',\n",
       " 'rack',\n",
       " 'wigger',\n",
       " 'college',\n",
       " 'Assclown',\n",
       " 'hun',\n",
       " 'smd',\n",
       " 'Bob',\n",
       " 'Loser',\n",
       " 'booty',\n",
       " 'Hood Rat',\n",
       " 'Pink Floyd',\n",
       " 'btw',\n",
       " 'blow job',\n",
       " 'Dubya',\n",
       " 'vurp',\n",
       " 'wet',\n",
       " 'motherfucker',\n",
       " 'sublime',\n",
       " 'bear',\n",
       " 'spange',\n",
       " 'Twink',\n",
       " 'spit roast',\n",
       " 'bird',\n",
       " 'TMI',\n",
       " 'microsoft',\n",
       " 'mom',\n",
       " 'mangina',\n",
       " 'nvm',\n",
       " 'peg',\n",
       " 'nut huggers',\n",
       " 'poser',\n",
       " 'First Base',\n",
       " 'indian',\n",
       " 'clitoris',\n",
       " 'wop',\n",
       " 'gaydar',\n",
       " 'cameltoe',\n",
       " 'whiskey dick',\n",
       " 'heroin',\n",
       " 'lit',\n",
       " 'Ecstasy',\n",
       " '42',\n",
       " 'joint',\n",
       " 'yayo',\n",
       " 'fucking',\n",
       " 'BFF',\n",
       " 'head',\n",
       " 'sexile',\n",
       " 'lmao',\n",
       " 'snitch',\n",
       " 'mofo',\n",
       " \"I'd Hit it\",\n",
       " 'BS',\n",
       " 'k',\n",
       " 'POPO',\n",
       " 'grill',\n",
       " 'Third Base',\n",
       " 'fugly',\n",
       " 'mob',\n",
       " 'vato',\n",
       " 'fuck buddy',\n",
       " 'sugar daddy',\n",
       " 'Nintendo',\n",
       " 'FYI',\n",
       " 'rugby',\n",
       " 'np',\n",
       " 'nookie',\n",
       " 'bellend',\n",
       " 'frontin',\n",
       " 'gooch',\n",
       " 'womyn',\n",
       " 'Peace',\n",
       " 'gank',\n",
       " 'cream',\n",
       " 'aka',\n",
       " 'IMHO',\n",
       " 'reggie',\n",
       " 'cosplay',\n",
       " 'Tits',\n",
       " 'DL',\n",
       " 'dime',\n",
       " 'disco nap',\n",
       " 'player',\n",
       " 'high maintenance',\n",
       " 'bitch slap',\n",
       " 'doobie',\n",
       " 'narc',\n",
       " 'Vegan',\n",
       " 'megadeth',\n",
       " 'Peeps',\n",
       " 'pedo',\n",
       " 'raw',\n",
       " 'chief',\n",
       " 'dylan',\n",
       " 'goatse',\n",
       " 'tease',\n",
       " '^_^',\n",
       " 'desu',\n",
       " 'dayum',\n",
       " 'idiot',\n",
       " 'homophobe',\n",
       " 'nsfw',\n",
       " 'toke',\n",
       " 'pillow talk',\n",
       " 'holy shit',\n",
       " 'ska',\n",
       " 'SOL',\n",
       " 'std',\n",
       " 'beef curtains',\n",
       " 'htf',\n",
       " 'Finland',\n",
       " 'Bitch',\n",
       " 'cuckold',\n",
       " 'front',\n",
       " 'SO',\n",
       " 'Tony Blair',\n",
       " 'teabagging',\n",
       " 'wet dream',\n",
       " 'My nigga',\n",
       " 'Hip Hop',\n",
       " 'CPT',\n",
       " 'terrorist',\n",
       " 'Blaze',\n",
       " 'raw dog',\n",
       " '187',\n",
       " \"ain't\",\n",
       " 'job',\n",
       " 'Tea Bag',\n",
       " 'stuck up',\n",
       " 'butters',\n",
       " 'g-string',\n",
       " 'sket',\n",
       " 'laid',\n",
       " 'prostitute',\n",
       " 'Incest',\n",
       " 'break the seal',\n",
       " 'lesbian',\n",
       " 'bra',\n",
       " 'tossing salad',\n",
       " 'dunno',\n",
       " \"y'all\",\n",
       " 'Flossy',\n",
       " 'chica',\n",
       " 'idk',\n",
       " 'glory hole',\n",
       " 'Muppet',\n",
       " 'filipino',\n",
       " 'lap dance',\n",
       " 'buttsex',\n",
       " 'LTR',\n",
       " 'orgy',\n",
       " 'scab',\n",
       " 'mids',\n",
       " 'Nug',\n",
       " 'Mexican Avalanche',\n",
       " 'nerf',\n",
       " 'aye',\n",
       " 'swoon',\n",
       " 'cleavland steamer',\n",
       " 'jailbait',\n",
       " 'o.g.',\n",
       " 'mate',\n",
       " 'sexual harassment',\n",
       " 'fuct',\n",
       " 'MoFo',\n",
       " 'afaik',\n",
       " 'Put me in coach!',\n",
       " 'Dump',\n",
       " 'cut a rug',\n",
       " 'Dime Piece',\n",
       " 'spade',\n",
       " 'Computer',\n",
       " 'po po',\n",
       " 'gams',\n",
       " 'Rape',\n",
       " 'w/e',\n",
       " 'oral sex',\n",
       " 'diss',\n",
       " 'faggotry',\n",
       " 'toss salad',\n",
       " 'strap',\n",
       " 'rimming',\n",
       " 'RIAA',\n",
       " 'trifling',\n",
       " 'beat',\n",
       " 'fox',\n",
       " 'Chubby',\n",
       " 'homeboy',\n",
       " 'chick',\n",
       " 'fronting',\n",
       " 'baked',\n",
       " 'THO',\n",
       " 'g spot',\n",
       " 'root',\n",
       " 'durf',\n",
       " 'it',\n",
       " 'CRS',\n",
       " 'BYOB',\n",
       " 'flow',\n",
       " 'hard on',\n",
       " 'bust-a-nut',\n",
       " 'Going Commando',\n",
       " 'Dutch Oven',\n",
       " 'bone',\n",
       " 'macking',\n",
       " 'cray cray',\n",
       " 'parents',\n",
       " 'hood',\n",
       " 'spick',\n",
       " 'ButtHurt',\n",
       " '...',\n",
       " 'gobshite',\n",
       " 'double penetration',\n",
       " 'grass',\n",
       " 'grope',\n",
       " 'dirty',\n",
       " 'scrap',\n",
       " 'reverse cowgirl',\n",
       " 'Pink',\n",
       " 'hung like a horse',\n",
       " 'Stacked',\n",
       " 'courtesy flush',\n",
       " '7 minutes in heaven',\n",
       " 'hit',\n",
       " 'ganja',\n",
       " 'do the damn thang',\n",
       " 'Deep Thrust',\n",
       " 'dro',\n",
       " 'chill',\n",
       " 'Booyah',\n",
       " 'Piper',\n",
       " 'sketch',\n",
       " 'bump',\n",
       " 'Condom',\n",
       " 'sammich',\n",
       " 'cheese',\n",
       " 'clap',\n",
       " 'jack off',\n",
       " 'oral',\n",
       " 'horney',\n",
       " 'spank',\n",
       " 'busted',\n",
       " 'fuckable',\n",
       " 'FWB',\n",
       " 'ripped',\n",
       " 'fudge',\n",
       " 'grower not a shower',\n",
       " 'Cats',\n",
       " 'Balls',\n",
       " 'succubus',\n",
       " 'Opeth',\n",
       " 'recursion',\n",
       " 'FUD',\n",
       " 'Muff Diving',\n",
       " 'giving head',\n",
       " 'Pearl Necklace',\n",
       " 'the rock',\n",
       " 'Hater',\n",
       " 'Muah',\n",
       " 'butterface',\n",
       " 'cold',\n",
       " 'cock tease',\n",
       " 'bimmer',\n",
       " 'shut up',\n",
       " 'ohio',\n",
       " 'tbh',\n",
       " 'omfg',\n",
       " 'shook',\n",
       " 'Swoll',\n",
       " 'Gays',\n",
       " 'awww',\n",
       " 'S',\n",
       " 'hydro',\n",
       " 'gleek',\n",
       " 'daisy chain',\n",
       " 'sissy',\n",
       " 'blow',\n",
       " 'jism',\n",
       " 'Diego',\n",
       " '411',\n",
       " 'bff',\n",
       " 'ttfn',\n",
       " 'bf',\n",
       " 'scrotum',\n",
       " 'legit',\n",
       " 'ball',\n",
       " 'Railed',\n",
       " 'plonker',\n",
       " 'fuckhole',\n",
       " 'grody',\n",
       " '143',\n",
       " 'Hood Rich',\n",
       " 'chop',\n",
       " 'nymphomaniac',\n",
       " 'dammit',\n",
       " 'Bbl',\n",
       " 'pinay',\n",
       " 'snail trail',\n",
       " 'fellatio',\n",
       " 'DAMAGED GOODS',\n",
       " 'FWIW',\n",
       " 'pinoy',\n",
       " 'scallywag',\n",
       " 'diane',\n",
       " 'bestiality',\n",
       " 'box',\n",
       " 'ghetto booty',\n",
       " 'black sabbath',\n",
       " 'FUBAR',\n",
       " 'sack',\n",
       " 'bucking bronco',\n",
       " 'newgrounds',\n",
       " '50 Cent',\n",
       " 'Dry humping',\n",
       " 'bull dyke',\n",
       " 'Cock Sucker',\n",
       " 'S.O.',\n",
       " \"poppin' tags\",\n",
       " 'Corey',\n",
       " 'water sports',\n",
       " 'gangster',\n",
       " 'camel-toe',\n",
       " 'bk',\n",
       " 'quarter',\n",
       " 'man crush',\n",
       " 'grind',\n",
       " 'lemming',\n",
       " 'paizuri',\n",
       " 'cam',\n",
       " 'wb',\n",
       " 'hubby',\n",
       " 'dog',\n",
       " 'mota',\n",
       " 'balls',\n",
       " 'reach around',\n",
       " 'mint',\n",
       " 'G',\n",
       " 'honey',\n",
       " 'NOFX',\n",
       " 'Roll',\n",
       " 'Nut',\n",
       " 'lurk',\n",
       " 'cum bucket',\n",
       " 'facial',\n",
       " 'ugly duckling syndrome',\n",
       " 'hotboxing',\n",
       " 'SCREW YOU',\n",
       " 'l33t',\n",
       " 'puma',\n",
       " 'book',\n",
       " 'mack',\n",
       " 'come',\n",
       " 'blue balls',\n",
       " 'sodomy',\n",
       " 'Conan',\n",
       " 'pron',\n",
       " 'Spanglish',\n",
       " 'mcdonalds',\n",
       " 'T_T',\n",
       " 'hymen',\n",
       " 'boonies',\n",
       " ';)',\n",
       " 'coconut',\n",
       " 'wtg',\n",
       " 'mouse off',\n",
       " 'mushroom stamp',\n",
       " 'X',\n",
       " 'skiing',\n",
       " 'awesome',\n",
       " 'Lena',\n",
       " 'ABC',\n",
       " 'Sucker',\n",
       " 'Kek',\n",
       " 'fag',\n",
       " 'tap that ass',\n",
       " 'Cowbell',\n",
       " ':P',\n",
       " 'anus',\n",
       " 'slit',\n",
       " 'Russian Roulette',\n",
       " 'blow me',\n",
       " 'tripping',\n",
       " 'LDR',\n",
       " 'fad',\n",
       " 'SHIV',\n",
       " 'grip',\n",
       " 'strap on',\n",
       " 'fgt',\n",
       " 'dickwad',\n",
       " 'turd burglar',\n",
       " 'ATL',\n",
       " 'cack',\n",
       " 'GQ',\n",
       " 'zip',\n",
       " 'rotfl',\n",
       " 'FRUIT',\n",
       " 'Frodo',\n",
       " 'TLDR',\n",
       " 'chocha',\n",
       " 'bork',\n",
       " 'Honda',\n",
       " 'five-o',\n",
       " 'vertical smile',\n",
       " 'assclown',\n",
       " 'Cheeba',\n",
       " 'Olsen Twins',\n",
       " 'fuck off',\n",
       " 'squirter',\n",
       " 'lbc',\n",
       " 'cush',\n",
       " 'rage',\n",
       " 'scouser',\n",
       " 'Brown',\n",
       " 'limey',\n",
       " 'hoser',\n",
       " 'rusty trumpet',\n",
       " '68',\n",
       " 'DD',\n",
       " 'kids',\n",
       " 'g string',\n",
       " 'leo',\n",
       " 'Anarchy',\n",
       " 'chunder',\n",
       " 'swirly',\n",
       " 'heffa',\n",
       " 'fucked up',\n",
       " 'porno',\n",
       " 'Morning wood',\n",
       " 'honey pot',\n",
       " 'Fuckwit',\n",
       " 'Chinese Food',\n",
       " 'snapper',\n",
       " 'ass clown',\n",
       " 'spiffy',\n",
       " 'Hell',\n",
       " 'philly',\n",
       " 'bonkers',\n",
       " 'GTFO',\n",
       " 'plow',\n",
       " 'wifebeater',\n",
       " 'secks',\n",
       " 'tag',\n",
       " 'Dutch oven',\n",
       " 'Brains',\n",
       " 'fapping',\n",
       " 'cumshot',\n",
       " 'yoked',\n",
       " 'rider',\n",
       " 'Bell End',\n",
       " 'orgasmic',\n",
       " 'fresh',\n",
       " 'locs',\n",
       " 'Gutta',\n",
       " 'fuck you',\n",
       " 'self-first',\n",
       " 'cheddar',\n",
       " 'walrus',\n",
       " 'todger',\n",
       " 'Fuck yeah',\n",
       " 'man in the boat',\n",
       " 'bunk',\n",
       " '8 ball',\n",
       " 'jk',\n",
       " '21',\n",
       " 'moxy',\n",
       " 'Suv',\n",
       " 'In-N-Out',\n",
       " 'ralph',\n",
       " 'fubu',\n",
       " 'go down on',\n",
       " 'Poonani',\n",
       " 'cornholio',\n",
       " 'beastality',\n",
       " 'kirk',\n",
       " 'nimrod',\n",
       " 'Feet',\n",
       " 'oy vey',\n",
       " 'wonky',\n",
       " 'jail bait',\n",
       " 'tenacious d',\n",
       " 'SOS',\n",
       " 'clean',\n",
       " 'quan',\n",
       " 'sexual',\n",
       " 'computer science',\n",
       " 'mario',\n",
       " 'kaya',\n",
       " 'nip slip',\n",
       " 'Mota',\n",
       " 'hotbox',\n",
       " 'do you',\n",
       " 'Boss',\n",
       " 'double d',\n",
       " 'sherm',\n",
       " 'Yenta',\n",
       " 'fark',\n",
       " 'Fack',\n",
       " 'dummy',\n",
       " 'def',\n",
       " 'rip',\n",
       " 'gusset typist',\n",
       " 'Russians',\n",
       " 'skanky',\n",
       " 'Biddies',\n",
       " '5th base',\n",
       " 'pinch a loaf',\n",
       " 'word up',\n",
       " 'dime bag',\n",
       " 'cocktease',\n",
       " 'Meep',\n",
       " 'Mustache Ride',\n",
       " 'Smegma',\n",
       " 'gj',\n",
       " 'titties',\n",
       " 'cop out',\n",
       " 'dry spell',\n",
       " 'wet back',\n",
       " 'Queer Bait',\n",
       " 'perv',\n",
       " 'swanky',\n",
       " 'ftp',\n",
       " 'hon',\n",
       " 'Trojan',\n",
       " 'pantsed',\n",
       " 'banjo string',\n",
       " 'pink taco',\n",
       " 'Snoop',\n",
       " 'slope',\n",
       " 'postal',\n",
       " 'silly goose',\n",
       " 'tsk',\n",
       " 'wimp',\n",
       " 'family',\n",
       " 'bling',\n",
       " 'fruit',\n",
       " 'bbs',\n",
       " 'sticky',\n",
       " '313',\n",
       " 'euphemism',\n",
       " 'Sancho',\n",
       " 'clusterfuck',\n",
       " 'addy',\n",
       " 'bearded clam',\n",
       " 'NORCAL',\n",
       " 'killer',\n",
       " 'k hole',\n",
       " 'batshit',\n",
       " 'Ginormous',\n",
       " 'pissant',\n",
       " 'drag',\n",
       " 'grub',\n",
       " 'TGIF',\n",
       " 'lesbo',\n",
       " 'Mushroom Stamp',\n",
       " 'sexually frustrated',\n",
       " 'dibs',\n",
       " 'buttfuck',\n",
       " 'carpet muncher',\n",
       " 'jonesing',\n",
       " 'cool beans',\n",
       " 'swede',\n",
       " 'zone',\n",
       " 'Nuzzle',\n",
       " 'pubes',\n",
       " 'toasted',\n",
       " 'game',\n",
       " 'lightweight',\n",
       " 'pitching a tent',\n",
       " 'HTH',\n",
       " 'wicked',\n",
       " 'jiz',\n",
       " 'bogus',\n",
       " \"T'aint\",\n",
       " 'catch you on the flip side',\n",
       " 'h town',\n",
       " 'twork',\n",
       " 'bollywood',\n",
       " 'camp',\n",
       " 'Reppin',\n",
       " 'nm',\n",
       " 'Balls Deep',\n",
       " 'fisting',\n",
       " 'Pigeon',\n",
       " 'vanilla',\n",
       " 'totes',\n",
       " 'gangbanger',\n",
       " 'annoying',\n",
       " 'moobs',\n",
       " 'pearl',\n",
       " 'ROTFLMAO',\n",
       " 'vegetable',\n",
       " 'Juju',\n",
       " 'baps',\n",
       " ':(',\n",
       " 'dipshit',\n",
       " 'bat wings',\n",
       " 'hardon',\n",
       " 'zen',\n",
       " 'ima',\n",
       " 'rube',\n",
       " 'butt fuck',\n",
       " 'size queen',\n",
       " 'ni',\n",
       " 'shanked',\n",
       " 'pimp juice',\n",
       " '^^',\n",
       " 'Toxic',\n",
       " 'totty',\n",
       " 'shakira',\n",
       " 'felch',\n",
       " 'Anarchist',\n",
       " 'uncut',\n",
       " 'clique',\n",
       " 'caboose',\n",
       " 'frag',\n",
       " 'kittens',\n",
       " 'tomato',\n",
       " 'fivehead',\n",
       " 'shitter',\n",
       " 'nincompoop',\n",
       " 'weasel',\n",
       " 'woop woop',\n",
       " 'chillin',\n",
       " 'manager',\n",
       " 'duh',\n",
       " 'eye candy',\n",
       " 'la',\n",
       " 'Henny',\n",
       " 'munted',\n",
       " 'nizzle',\n",
       " 'dick slap',\n",
       " 'balloon knot',\n",
       " 'twisted',\n",
       " 'hot damn',\n",
       " 'joshing',\n",
       " 'get laid',\n",
       " 'loot',\n",
       " 'aqua teen hunger force',\n",
       " 'toe jam',\n",
       " 'Poontang',\n",
       " 'Man Up',\n",
       " 'boondocks',\n",
       " 'lifted',\n",
       " 'fyad',\n",
       " 'ROFL',\n",
       " 'shits and giggles',\n",
       " 'wack',\n",
       " 'lesbians',\n",
       " 'lubricant',\n",
       " 'esse',\n",
       " 'rents',\n",
       " 'frog',\n",
       " 'Frig',\n",
       " 'o.O',\n",
       " 'KIT',\n",
       " 'rico suave',\n",
       " 'Pfft',\n",
       " 'fag hag',\n",
       " '69 position',\n",
       " 'bitch make me a sandwich',\n",
       " 'weak',\n",
       " 'axe',\n",
       " 'raped',\n",
       " 'hot monkey sex',\n",
       " 'scratch',\n",
       " '666',\n",
       " 'diesel',\n",
       " 'Weiner',\n",
       " 'whigger',\n",
       " 'hf',\n",
       " 'horse cock',\n",
       " 'Morning head',\n",
       " 'mug',\n",
       " 'r kelly',\n",
       " 'KFC',\n",
       " 'Credit Card',\n",
       " 'one eyed pirate',\n",
       " 'ttt',\n",
       " 'afro',\n",
       " 'corny',\n",
       " 'drop trou',\n",
       " 'spanish fly',\n",
       " 'bowl',\n",
       " 'fuckbuddy',\n",
       " 'blown',\n",
       " 'prison bitch',\n",
       " 'Harry Pothead',\n",
       " 'angel dust',\n",
       " 'Chillax',\n",
       " 'numpty',\n",
       " 'big ups',\n",
       " 'pooch',\n",
       " 'hot beef injection',\n",
       " 'Snoopy',\n",
       " 'rere',\n",
       " 'bumblefuck',\n",
       " 'duckets',\n",
       " 'pee',\n",
       " '4',\n",
       " 'pound',\n",
       " 'dough',\n",
       " 'panocha',\n",
       " 'gaysian',\n",
       " 'Booze',\n",
       " 'wife',\n",
       " 'bfg',\n",
       " 'cyber sex',\n",
       " 'creampie',\n",
       " 'onion',\n",
       " 'Mack Daddy',\n",
       " 'sif',\n",
       " 'cockslap',\n",
       " 'clown',\n",
       " 'queaf',\n",
       " 'nob',\n",
       " 'oh shit bar',\n",
       " 'nipples',\n",
       " 'toss the salad',\n",
       " 'bammer',\n",
       " 'milking',\n",
       " 'politics',\n",
       " 'wall',\n",
       " 'piracy',\n",
       " 'slug',\n",
       " 'mang',\n",
       " 'Darn',\n",
       " 'baby',\n",
       " 'tweaking',\n",
       " 'yep',\n",
       " 'no worries',\n",
       " 'chomo',\n",
       " 'smash',\n",
       " 'cabbage',\n",
       " 'Fucking the dog',\n",
       " 'fin',\n",
       " 'nooner',\n",
       " 'cum shot',\n",
       " 'NPC',\n",
       " 'robbing the cradle',\n",
       " 'pistol whip',\n",
       " 'hesher',\n",
       " 'PTFO',\n",
       " 'defenestrate',\n",
       " 'puppies',\n",
       " 'Member',\n",
       " 'snowflake',\n",
       " 'elbow',\n",
       " 'chunk the deuce',\n",
       " 'Bri',\n",
       " 'rass',\n",
       " 'D.A.R.E.',\n",
       " 'Cocksucker',\n",
       " 'set',\n",
       " 'whores',\n",
       " 'Light in the loafers',\n",
       " 'beating around the bush',\n",
       " 'buggery',\n",
       " 'bbbj',\n",
       " 'check',\n",
       " 'Mother Fucker',\n",
       " 'I feel you',\n",
       " 'Crescent Fresh',\n",
       " 'Kid Rock',\n",
       " 'SMIB',\n",
       " 'bugaboo',\n",
       " 'penis art',\n",
       " 'Pearl Jam',\n",
       " 'pussy juice',\n",
       " 'Navid',\n",
       " 'YHBT',\n",
       " 'dirt',\n",
       " 'Tea-Bag',\n",
       " 'cock gobbler',\n",
       " 'boobs',\n",
       " 'dune coon',\n",
       " 'cockpit',\n",
       " ':D',\n",
       " 'titfuck',\n",
       " 'Asian persuasion',\n",
       " 'reefer',\n",
       " 'white russian',\n",
       " 'stoge',\n",
       " 'True',\n",
       " 'Fuck Stick',\n",
       " 'fufu',\n",
       " 'gg no re',\n",
       " 'felcher',\n",
       " 'Feltcher',\n",
       " 'nose candy',\n",
       " 'beej',\n",
       " 'corn hole',\n",
       " 'spongebob squarepants',\n",
       " 'stash',\n",
       " 'busking',\n",
       " 'bunt',\n",
       " 'No Diggity',\n",
       " 'Sea',\n",
       " 'Verbal Diarrhea',\n",
       " 'faghag',\n",
       " 'government',\n",
       " 'RTS',\n",
       " 'truffle shuffle',\n",
       " 'tea bag',\n",
       " 'dicked down',\n",
       " 'bees knees',\n",
       " 'cock block',\n",
       " 'gf',\n",
       " 'gusher',\n",
       " 'pork sword',\n",
       " 'slangin',\n",
       " 'chop it up',\n",
       " 'Gorillaz',\n",
       " 'chillaxin',\n",
       " 'weirdo',\n",
       " 'Beaner',\n",
       " 'toe',\n",
       " 'Coochie',\n",
       " 'dingaling',\n",
       " 'HOME WRECKER.',\n",
       " 'Marching Band',\n",
       " 'pour one for my homies',\n",
       " 'sapiosexuality',\n",
       " 'book smart',\n",
       " 'liek',\n",
       " 'chuffed',\n",
       " 'kurt',\n",
       " 'Poo',\n",
       " 'jawn',\n",
       " 'scrilla',\n",
       " 'homeslice',\n",
       " 'Whiskey Tango',\n",
       " 'mardy',\n",
       " 'camel jockey',\n",
       " 'DROP TOP',\n",
       " 'nathen',\n",
       " 'brill',\n",
       " 'Asian Fever',\n",
       " 'Beastie Boys',\n",
       " 'na mean',\n",
       " 'GOT YOUR BACK',\n",
       " 'pum pum',\n",
       " 'BFD',\n",
       " 'brother',\n",
       " 'Snowball',\n",
       " 'funky',\n",
       " 'Bukakke',\n",
       " 'Pillock',\n",
       " 'precum',\n",
       " 'beer shits',\n",
       " 'Freegan',\n",
       " 'mean muggin',\n",
       " 'U2',\n",
       " 'rock',\n",
       " 'yahoo',\n",
       " 'elite',\n",
       " 'pop music',\n",
       " 'Clam Bake',\n",
       " 'pen',\n",
       " 'Mango',\n",
       " 'nards',\n",
       " 'dimwit',\n",
       " 'camper',\n",
       " 'missionary position',\n",
       " 'Baby gravy',\n",
       " 'Jabroni',\n",
       " 'jigga',\n",
       " 'outta',\n",
       " 'Frienemy',\n",
       " 'algebra',\n",
       " 'robotrip',\n",
       " 'token black guy',\n",
       " 'tweak',\n",
       " 'pusher',\n",
       " 'Alize',\n",
       " 'NWS',\n",
       " 'Underground',\n",
       " 'darkies',\n",
       " 'weener',\n",
       " 'mature',\n",
       " 'J',\n",
       " 'prego',\n",
       " 'Pie Hole',\n",
       " 'Young',\n",
       " '211',\n",
       " 'feeling up',\n",
       " 'muffdiver',\n",
       " 'jive',\n",
       " 'harlem shake',\n",
       " 'rat',\n",
       " 'high beams',\n",
       " 'skullet',\n",
       " 'right on',\n",
       " 'tough',\n",
       " 'sick',\n",
       " 'arvo',\n",
       " 'for real',\n",
       " 'Piss ant',\n",
       " 'beat off',\n",
       " 'mummy',\n",
       " 'taint',\n",
       " 'salad tosser']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make list out of dico\n",
    "dicolist= dico[\"word\"].unique().tolist()\n",
    "dicolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "776c39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset sample\n",
    "\n",
    "#Jules : \n",
    "#path=r'/Users/jules/ADA_project_data/Sample_cleaned_1Mio.json.bz2'\n",
    "\n",
    "#Nico: '/Users/nicolasantacroce/Desktop/Desktop/EPFL/EPFL MA1/Applied Data Analysis/Sample.json.bz2'\n",
    "df= pd.read_json('C:/Users/Nicola/Desktop/EPFL/MA1/ADA/Sample.json.bz2',compression=\"bz2\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b8ade72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-11-109291</td>\n",
       "      <td>They'll call me lots of different things. Libe...</td>\n",
       "      <td>Chris Christie</td>\n",
       "      <td>[Q63879]</td>\n",
       "      <td>2015-11-11 00:55:12</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Chris Christie, 0.7395], [Bobby Jindal, 0.15...</td>\n",
       "      <td>[http://thehill.com/blogs/ballot-box/259760-ch...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-04-105046</td>\n",
       "      <td>The choices are not that easy,</td>\n",
       "      <td>Dr. John</td>\n",
       "      <td>[Q511074, Q54593093]</td>\n",
       "      <td>2015-11-04 18:13:06</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Dr. John, 0.5531], [None, 0.4469]]</td>\n",
       "      <td>[http://delawareonline.com/story/news/health/2...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-11-070666</td>\n",
       "      <td>It's kind of the same way it's been with the R...</td>\n",
       "      <td>Niklas Kronwall</td>\n",
       "      <td>[Q722939]</td>\n",
       "      <td>2015-09-11 19:54:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Niklas Kronwall, 0.7119], [None, 0.2067], [H...</td>\n",
       "      <td>[http://redwings.nhl.com/club/news.htm?id=7787...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-12-082489</td>\n",
       "      <td>We're now going back to the frozen tundra, and...</td>\n",
       "      <td>Frances McDormand</td>\n",
       "      <td>[Q204299]</td>\n",
       "      <td>2015-01-12 01:40:00</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Frances McDormand, 0.484], [None, 0.4495], [...</td>\n",
       "      <td>[http://feeds.people.com/~r/people/headlines/~...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-09-033345</td>\n",
       "      <td>I had a chuckle: They were showing a video of ...</td>\n",
       "      <td>Kris Draper</td>\n",
       "      <td>[Q948695]</td>\n",
       "      <td>2015-11-09 00:57:45</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Kris Draper, 0.8782], [None, 0.1043], [Serge...</td>\n",
       "      <td>[http://ca.rd.yahoo.com/sports/rss/nfl/SIG=13u...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-11-11-109291  They'll call me lots of different things. Libe...   \n",
       "1  2015-11-04-105046                     The choices are not that easy,   \n",
       "2  2015-09-11-070666  It's kind of the same way it's been with the R...   \n",
       "3  2015-01-12-082489  We're now going back to the frozen tundra, and...   \n",
       "4  2015-11-09-033345  I had a chuckle: They were showing a video of ...   \n",
       "\n",
       "             speaker                  qids                date  \\\n",
       "0     Chris Christie              [Q63879] 2015-11-11 00:55:12   \n",
       "1           Dr. John  [Q511074, Q54593093] 2015-11-04 18:13:06   \n",
       "2    Niklas Kronwall             [Q722939] 2015-09-11 19:54:00   \n",
       "3  Frances McDormand             [Q204299] 2015-01-12 01:40:00   \n",
       "4        Kris Draper             [Q948695] 2015-11-09 00:57:45   \n",
       "\n",
       "   numOccurrences                                             probas  \\\n",
       "0               1  [[Chris Christie, 0.7395], [Bobby Jindal, 0.15...   \n",
       "1               2               [[Dr. John, 0.5531], [None, 0.4469]]   \n",
       "2               1  [[Niklas Kronwall, 0.7119], [None, 0.2067], [H...   \n",
       "3               3  [[Frances McDormand, 0.484], [None, 0.4495], [...   \n",
       "4               3  [[Kris Draper, 0.8782], [None, 0.1043], [Serge...   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://thehill.com/blogs/ballot-box/259760-ch...     E  \n",
       "1  [http://delawareonline.com/story/news/health/2...     E  \n",
       "2  [http://redwings.nhl.com/club/news.htm?id=7787...     E  \n",
       "3  [http://feeds.people.com/~r/people/headlines/~...     E  \n",
       "4  [http://ca.rd.yahoo.com/sports/rss/nfl/SIG=13u...     E  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58aa2524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83feb898c8cf4d109538994e7b9a2aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicola\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4249",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 4249",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9580/646692068.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#separating quotes (really long so we try it on the first 10000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"colloquial\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quotation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0misinside1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdicolist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    812\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9580/646692068.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#separating quotes (really long so we try it on the first 10000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"colloquial\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quotation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0misinside1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdicolist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9580/2330238185.py\u001b[0m in \u001b[0;36misinside1\u001b[1;34m(test_string, test_list)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mele\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mdico\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"occurences\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdico\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"occurences\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3774\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3775\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3776\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3778\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adaenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 4249"
     ]
    }
   ],
   "source": [
    "#separating quotes (really long so we try it on the first 10000)\n",
    "df2= df[:10000]\n",
    "df2[\"colloquial\"]= df2[\"quotation\"].progress_apply(lambda x : isinside1(x,dicolist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2075013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>colloquial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.260400</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.155279</td>\n",
       "      <td>0.205992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1125.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       numOccurrences    colloquial\n",
       "count    10000.000000  10000.000000\n",
       "mean         3.260400      0.955600\n",
       "std         18.155279      0.205992\n",
       "min          1.000000      0.000000\n",
       "25%          1.000000      1.000000\n",
       "50%          1.000000      1.000000\n",
       "75%          2.000000      1.000000\n",
       "max       1125.000000      1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "217154c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurences</th>\n",
       "      <th>strlen</th>\n",
       "      <th>occurence_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16339.000000</td>\n",
       "      <td>16339.000000</td>\n",
       "      <td>16339.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.178652</td>\n",
       "      <td>9.643858</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>74.463508</td>\n",
       "      <td>5.264567</td>\n",
       "      <td>0.007446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6573.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         occurences        strlen  occurence_fraction\n",
       "count  16339.000000  16339.000000        16339.000000\n",
       "mean       3.178652      9.643858            0.000318\n",
       "std       74.463508      5.264567            0.007446\n",
       "min        0.000000      1.000000            0.000000\n",
       "25%        0.000000      6.000000            0.000000\n",
       "50%        0.000000      9.000000            0.000000\n",
       "75%        0.000000     12.000000            0.000000\n",
       "max     6573.000000    143.000000            0.657300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing words present in more than 0.1% of quotes\n",
    "tresh= 0.001\n",
    "dico[\"occurence_fraction\"]= dico[\"occurences\"]/df2[\"colloquial\"].count()\n",
    "dico.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e6288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurences</th>\n",
       "      <th>strlen</th>\n",
       "      <th>occurence_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16142.000000</td>\n",
       "      <td>16142.000000</td>\n",
       "      <td>16142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.074960</td>\n",
       "      <td>9.715834</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.545739</td>\n",
       "      <td>5.252921</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         occurences        strlen  occurence_fraction\n",
       "count  16142.000000  16142.000000        16142.000000\n",
       "mean       0.074960      9.715834            0.000007\n",
       "std        0.545739      5.252921            0.000055\n",
       "min        0.000000      1.000000            0.000000\n",
       "25%        0.000000      6.000000            0.000000\n",
       "50%        0.000000      9.000000            0.000000\n",
       "75%        0.000000     12.000000            0.000000\n",
       "max        9.000000    143.000000            0.000900"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating new dictionary without most occuring words\n",
    "dico2= dico[dico[\"occurence_fraction\"]<tresh]\n",
    "dicolist2= dico2[\"word\"].unique().tolist()\n",
    "dico2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "093eccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eb0766ef7b4cc79c29128e98f96337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/9ks4sslj4hj9kg36pxlnc3xc0000gn/T/ipykernel_56249/745375064.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[\"colloquial\"]= df2[\"quotation\"].progress_apply(lambda x : isinside2(x,dicolist2))\n"
     ]
    }
   ],
   "source": [
    "#re-evaluating quotes with reduced dictionary\n",
    "df2[\"colloquial\"]= df2[\"quotation\"].progress_apply(lambda x : isinside2(x,dicolist2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc299d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>colloquial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.260400</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.155279</td>\n",
       "      <td>0.306826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1125.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       numOccurrences    colloquial\n",
       "count    10000.000000  10000.000000\n",
       "mean         3.260400      0.105200\n",
       "std         18.155279      0.306826\n",
       "min          1.000000      0.000000\n",
       "25%          1.000000      0.000000\n",
       "50%          1.000000      0.000000\n",
       "75%          2.000000      0.000000\n",
       "max       1125.000000      1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "feaeffcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It seems to be that every time effective investigators like the ones we work with begin to turn over rocks, they find creepy crawly things,']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_colloquials= df2[df2[\"colloquial\"]==1]\n",
    "df_colloquials[[\"quotation\"]].sample()[\"quotation\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "439ff793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurences</th>\n",
       "      <th>strlen</th>\n",
       "      <th>occurence_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>Sweet Ass Maria Sally Muzzone</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>Dutch Oven</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>coucher</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15428</th>\n",
       "      <td>scrub up well</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8893</th>\n",
       "      <td>Chiparati</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>speen</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14413</th>\n",
       "      <td>kick off</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>D-ware</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>ashey</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10276</th>\n",
       "      <td>fo' shizzle</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9407</th>\n",
       "      <td>YuCON</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>airy-fairy</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12546</th>\n",
       "      <td>Logjammer</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874</th>\n",
       "      <td>no way Jos!</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>TLB</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14757</th>\n",
       "      <td>mug's game</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>scratchies</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10056</th>\n",
       "      <td>elvasate</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>blapse</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>torok</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                word  occurences  strlen  occurence_fraction\n",
       "11994  Sweet Ass Maria Sally Muzzone           0      29                 0.0\n",
       "5287                      Dutch Oven           0      10                 0.0\n",
       "359                          coucher           0       7                 0.0\n",
       "15428                  scrub up well           0      13                 0.0\n",
       "8893                       Chiparati           0       9                 0.0\n",
       "4861                           speen           0       5                 0.0\n",
       "14413                      kick off            0       9                 0.0\n",
       "6165                          D-ware           0       6                 0.0\n",
       "11396                          ashey           0       5                 0.0\n",
       "10276                    fo' shizzle           0      11                 0.0\n",
       "9407                           YuCON           0       5                 0.0\n",
       "12925                    airy-fairy            0      11                 0.0\n",
       "12546                      Logjammer           0       9                 0.0\n",
       "14874                    no way Jos!           0      11                 0.0\n",
       "10244                            TLB           0       3                 0.0\n",
       "14757                    mug's game            0      11                 0.0\n",
       "74                        scratchies           0      10                 0.0\n",
       "10056                       elvasate           0       8                 0.0\n",
       "7453                          blapse           0       6                 0.0\n",
       "9261                           torok           0       5                 0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico2.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8d91d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurences</th>\n",
       "      <th>strlen</th>\n",
       "      <th>occurence_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word, occurences, strlen, occurence_fraction]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico2[dico2[\"word\"]==\"Fuck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dd0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
