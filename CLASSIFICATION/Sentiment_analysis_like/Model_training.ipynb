{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a700234",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2356545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /Users/nicolasantacroce/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"subjectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3102ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8b683",
   "metadata": {},
   "source": [
    "## Tutorial part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae94a3",
   "metadata": {},
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "#subj_docs[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011006ae",
   "metadata": {},
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f54981",
   "metadata": {},
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words(training_docs,labeled= True)\n",
    "#all_words_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94c2ce",
   "metadata": {},
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "#unigram_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a45235",
   "metadata": {},
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "#test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470dcad",
   "metadata": {},
   "source": [
    "print(testing_docs[0][1],testing_docs[9][1],testing_docs[19][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db612fd",
   "metadata": {},
   "source": [
    "#trainer = NaiveBayesClassifier.train\n",
    "#classifier = sentim_analyzer.train(trainer, training_set)\n",
    "NBC= NaiveBayesClassifier.train(training_set)\n",
    "NBC.classify_many([test_set[0][0],test_set[9][0],test_set[19][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6e61e",
   "metadata": {},
   "source": [
    "## Formal/imformal model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b160bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "informal_corpus= \"./soap-text.txt\"\n",
    "formal_corpus= \"./wikipedia-text.txt\"\n",
    "model_name= \"NBC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8693ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading soap operas text\n",
    "soap_file = open(informal_corpus, \"r\")\n",
    "soap_text = soap_file.read() # string\n",
    "soap_file.close()\n",
    "# tokenizing sentences using punctuations\n",
    "punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "soap_sentences= (punkt_tokenizer.tokenize(soap_text))\n",
    "# tokenizing sentences using spaces (and removal of words containing \"@\")\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "informal_docs= [] # list of tuples, tuples contain tokens and label\n",
    "for sentence in soap_sentences:\n",
    "    tokenized_sentence= space_tokenizer.tokenize(sentence)\n",
    "    if len(tokenized_sentence)>1:\n",
    "        informal_docs.append(([ele for ele in tokenized_sentence if not (re.search('[@#/<>*]', ele))],\"informal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0063b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading wikipedia articles text\n",
    "wiki_file = open(formal_corpus, \"r\")\n",
    "wiki_text = wiki_file.read() # string\n",
    "wiki_file.close()\n",
    "# tokenizing sentences using punctuations\n",
    "punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "wiki_sentences= (punkt_tokenizer.tokenize(wiki_text))\n",
    "# tokenizing sentences using spaces (and removal of words containing \"@\")\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "formal_docs= [] # list of tuples, tuples contain tokens and label\n",
    "for sentence in wiki_sentences:\n",
    "    tokenized_sentence= space_tokenizer.tokenize(sentence)\n",
    "    if len(tokenized_sentence)>1: # no single word sentences\n",
    "        formal_docs.append(([ele for ele in tokenized_sentence if not (re.search('[@#/<>*]', ele))],\"formal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fd75943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formal, test_formal = nltk.sentiment.util.split_train_test(formal_docs, n=None)\n",
    "train_informal, test_informal = nltk.sentiment.util.split_train_test(informal_docs, n=None)\n",
    "train_docs= train_formal+train_informal\n",
    "test_docs= test_formal+test_informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eed4775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words = sentim_analyzer.all_words(train_docs,labeled= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb5aabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6efc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8ed4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = sentim_analyzer.apply_features(train_docs)\n",
    "test_set = sentim_analyzer.apply_features(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec1a9693",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ll/9ks4sslj4hj9kg36pxlnc3xc0000gn/T/ipykernel_35408/4239788168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNBC\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mNBC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/adaenv/lib/python3.8/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/adaenv/lib/python3.8/site-packages/nltk/collections.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/adaenv/lib/python3.8/site-packages/nltk/classify/util.py\u001b[0m in \u001b[0;36mlazy_func\u001b[0;34m(labeled_token)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlazy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeature_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/adaenv/lib/python3.8/site-packages/nltk/sentiment/sentiment_analyzer.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/adaenv/lib/python3.8/site-packages/nltk/sentiment/util.py\u001b[0m in \u001b[0;36mextract_unigram_feats\u001b[0;34m(document, unigrams, handle_negation)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmark_negation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"contains({word})\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NBC= NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ee1e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "for i in range(1000):\n",
    "    if NBC.classify(test_set[i][0])==test_docs[i][1]:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af1e84e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f229564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(model_name+'_model.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(NBC, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "efe85eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_name+\"_features.pkl\",\"wb\") as file:\n",
    "    pickle.dump(sentim_analyzer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
