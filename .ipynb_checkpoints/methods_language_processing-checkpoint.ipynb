{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3babc88",
   "metadata": {},
   "source": [
    "# Method 1 TF/IDF (simple)\n",
    "\n",
    "First method consist of TF/IDF ranking. \n",
    "\n",
    "It is based on a vector space retrieval (VSR) model. For testing the model is used on a simple document collection with a dozen of documents in file *blabla.txt*:\n",
    "\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | I think therefore I am\n",
    "  2     | Climate change is a hoax\n",
    "  3     | Fakenews\n",
    "  4     | More beautiful life\n",
    "  5     | Jules and Mr. Gilleron are very friendly\n",
    "  6     | Road to Minor improvement\n",
    "  7     | In love with the Rolex meeting rooms\n",
    "  8     | The important thing is to laugh\n",
    "  9     | A small step for man but a big step for humanity\n",
    " \n",
    " In information retrieval, tf–idf, TF\\*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. For more info: [link](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db7a554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary in the documents\n",
      " ------------------------------\n",
      " ['a', 'beauti', 'big', 'chang', 'climat', 'fakenew', 'friendli', 'gilleron', 'hoax', 'human', 'i', 'import', 'improv', 'in', 'jule', 'laugh', 'life', 'love', 'man', 'meet', 'minor', 'more', 'mr', 'road', 'rolex', 'room', 'small', 'step', 'the', 'therefor', 'thing', 'think']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"blabla.txt\", encoding='utf8') as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()\n",
    "print('Vocabulary in the documents\\n ------------------------------\\n', vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "950064c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6829d",
   "metadata": {},
   "source": [
    "Now, for the query $Q$ = $love$, here the top ranked documents according to the TF/IDF rank using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a36c4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correspondances: 2\n",
      "----------------------------\n",
      "Fakenews\n",
      "Jules and Mr. Gilleron are very friendly\n"
     ]
    }
   ],
   "source": [
    "ret_ids = search_vec_sklearn('Jules Fakenews', features)\n",
    "print('Found correspondances:', len(ret_ids))\n",
    "print('----------------------------')\n",
    "for i, v in enumerate(ret_ids):\n",
    "    print(original_documents[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2bc9f",
   "metadata": {},
   "source": [
    "# Method 2: TF/IDF + Relevance Feedback\n",
    "\n",
    "Implementation and test of [Rocchio's method](https://en.wikipedia.org/wiki/Rocchio_algorithm) for user relevance feedback. Like many other retrieval systems, the Rocchio feedback approach was developed using the Vector Space Model. The algorithm is based on the assumption that most users have a general conception of which documents should be denoted as relevant or non-relevant. Therefore, the user's search query is revised to include an arbitrary percentage of relevant and non-relevant documents as a means of increasing the search engine's recall, and possibly the precision as well. The number of relevant and non-relevant documents allowed to enter a query is dictated by the weights of the a, b, c variables listed below in the Algorithm section.\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_j} \\in D_{nr}} \\vec{d_j}\n",
    "\\end{equation}\n",
    "In the Rocchio algorithm negative term weights are ignored. This means, for the negative term weights in $\\vec{q_m}$, we set them to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d388488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "#with open(\"bread.txt\") as f:\n",
    "with open(\"blabla.txt\", encoding = 'utf8') as f:\n",
    "    content = f.readlines()\n",
    "#original_documents = [x.decode('utf-8').strip() for x in content] # for python2\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd39f43",
   "metadata": {},
   "source": [
    "Shaping the modified vector in a direction closer, or farther away, from the original query, related documents, and non-related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67ab1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "\n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = query_vector*alpha\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = [beta*sum(x)/num_rel for x in zip(*relevant_doc_vecs)]\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = [-gamma*sum(x)/num_non_rel for x in zip(*non_relevant_doc_vecs)]\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = [sum(x) for x in zip(norm_sum_relevant, norm_sum_non_relevant, norm_query_vector)]\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = [x if x>0 else 0 for x in modified_query_vector]\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce6de29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 In love with the Rolex meeting rooms\n",
      "1 Jules and Mr. Gilleron are very friendly\n",
      "2 I think therefore I am\n",
      "3 Climate change is a hoax\n",
      "4 Fakenews\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"Jules loves paragliding\", 5)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39bb6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  friendli gilleron jule love meet mr rolex room therefor think veri\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jules and Mr. Gilleron are very friendly',\n",
       " 'In love with the Rolex meeting rooms',\n",
       " 'I think therefore I am',\n",
       " 'Climate change is a hoax',\n",
       " 'Fakenews',\n",
       " 'More beautiful life',\n",
       " 'Road to Minor improvement',\n",
       " 'The important thing is to laugh',\n",
       " 'A small step for man but a big step for humanity']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "# suppose first three documents were relevant and the rest were irrelevant.\n",
    "relevant_indices = [0,1,2]\n",
    "non_relevant_indices = [i for i in range(3, len(ans))]\n",
    "\n",
    "relevant_doc_ids = [result_doc_ids[i] for i in relevant_indices]\n",
    "non_relevant_doc_ids = [result_doc_ids[i] for i in non_relevant_indices]\n",
    "\n",
    "relevant_doc_vecs = [document_vectors[i] for i in relevant_doc_ids]\n",
    "non_relevant_doc_vecs = [document_vectors[i] for i in non_relevant_doc_ids]\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , not_important_now, idontcare_anymore = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07319e",
   "metadata": {},
   "source": [
    "# Method 3: Word embeddings\n",
    "In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "For this method, we would train word embeddings using a state-of-the-art embeddings [library fastText](https://ai.facebook.com/tools/fasttext) (facebook).\n",
    "\n",
    "Note that it is only available on linux and iOS :( $\\rightarrow$ Virtual Machine or Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('epfldocs.txt', model = 'cbow')\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aeb522",
   "metadata": {},
   "source": [
    "\n",
    "### Visualization of the Embeddings\n",
    "\n",
    "Visualization the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities). t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') \n",
    "vis_data = tsne.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ec7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ca938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
