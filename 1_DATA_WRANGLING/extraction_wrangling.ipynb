{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bb0c4f",
   "metadata": {},
   "source": [
    "# Data final extraction\n",
    "With data wrangling inside the sample creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d33b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required modules\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f34bb2-33a6-4ab7-b7a4-91bf226d1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of years available and used\n",
    "years = [2015,2016,2017,2018,2019,2020]\n",
    "\n",
    "# Size of years file in GB\n",
    "year_size = np.array([3.3,2.3,5.2,4.8,3.6,.9])\n",
    "\n",
    "# Number of rows in file of 2020 (computed once \"by hand\", as long as this file is not too heavy)\n",
    "rows2020 = 5244449\n",
    "\n",
    "# Estimation of number of rows in each file\n",
    "year_rows = year_size*rows2020/.9\n",
    "\n",
    "# Chunk sizes\n",
    "chunk_sizes = [1e6,1e5,1e5,1e5,1e5,1e5]\n",
    "\n",
    "# Which correspond to n chunks of size one million\n",
    "chunks_number = np.rint(year_rows/chunk_sizes).astype(int)\n",
    "\n",
    "# Which means we have to take n random rows in each chunk to get a sample of 1,600,000 rows per year\n",
    "rows_in_chunk = np.rint(1.6*1e6/chunks_number).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53fbaf-2122-4004-87fe-41e3a0ea11d3",
   "metadata": {},
   "source": [
    "Sanity check :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d669bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([84211, 11940,  5281,  5714,  7619, 30769]),\n",
       " array([ 19, 134, 303, 280, 210,  52]),\n",
       " array([1600009, 1599960, 1600143, 1599920, 1599990, 1599988]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_in_chunk, chunks_number, rows_in_chunk*chunks_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0979e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files all have the following columns\n",
    "Index = ['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase','p1','p2','delta_p','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d7bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function, when testing loops\n",
    "def process_chunk(chunk,year):\n",
    "        print(f'Processing chunk with {len(chunk)} rows, in file from year {year}')\n",
    "        # print(chunk.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d93b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est parti, mon kiki !\n",
      "\n",
      "Start year 2015 with chunks of size 1000000\n",
      "Sanitiy check : len of df = 1768431\n",
      "Done with year 2015\n",
      "Start year 2016 with chunks of size 100000\n",
      "Sanitiy check : len of df = 1659660\n",
      "Done with year 2016\n",
      "Start year 2017 with chunks of size 100000\n",
      "Sanitiy check : len of df = 1410027\n",
      "Done with year 2017\n",
      "Start year 2018 with chunks of size 100000\n",
      "Sanitiy check : len of df = 1559922\n",
      "Done with year 2018\n",
      "Start year 2019 with chunks of size 100000\n",
      "Sanitiy check : len of df = 1660942\n",
      "Done with year 2019\n",
      "Start year 2020 with chunks of size 100000\n",
      "Sanitiy check : len of df = 1623688\n",
      "Done with year 2020\n",
      "\n",
      "Tout est bien qui finit bien\n"
     ]
    }
   ],
   "source": [
    "#fixing the thresholds (=> to low percentage, resp. percentage difference, of attribution to be considered in the analysis)\n",
    "threshold_min = 0.5\n",
    "threshold_diff = 0.3\n",
    "\n",
    "\n",
    "print(\"C'est parti, mon kiki !\\n\")\n",
    "\n",
    "for iii, year in enumerate(years) :\n",
    "    with pd.read_json(f'./Quotebank/quotes-{year}.json.bz2', lines=True, compression='bz2', chunksize=chunk_sizes[iii]) as df_reader:\n",
    "        print(f\"Start year {year} with chunks of size {int(chunk_sizes[iii])}\")\n",
    "        df = pd.DataFrame(columns=Index)\n",
    "        for chunk in df_reader:\n",
    "            \n",
    "            # Take only rows with at least two possible speakers\n",
    "            IndexP1 = chunk[chunk['probas'].str.len() < 2].index\n",
    "            chunk = chunk.drop(IndexP1 , inplace=False)\n",
    "            \n",
    "            #highest probability\n",
    "            chunk['p1'] = [i[0][1] for i in chunk['probas']]\n",
    "            chunk['p1'] = chunk['p1'].astype(float)\n",
    "\n",
    "            #second highest probability\n",
    "            chunk['p2'] = [i[1][1] for i in chunk['probas']]\n",
    "            chunk['p2'] = chunk['p2'].astype(float)\n",
    "\n",
    "            #difference between the two above\n",
    "            chunk['delta_p'] = chunk['p1']-chunk['p2']\n",
    "\n",
    "            #extracting the date\n",
    "            chunk['year'] = pd.DatetimeIndex(chunk['date']).year\n",
    "            \n",
    "            \n",
    "            # Removing the rows that not pass the criterions\n",
    "            indexNames = chunk[\n",
    "                                    (chunk['p1'] < threshold_min)\n",
    "                                  | (chunk['delta_p'] < threshold_diff)\n",
    "                                  | (chunk['speaker'] == 'None')\n",
    "                                                                                      ].index\n",
    "\n",
    "            chunk = chunk.drop(indexNames , inplace=False)\n",
    "            \n",
    "            df = pd.concat([df,chunk.sample(min(rows_in_chunk[iii],len(chunk)))])\n",
    "        \n",
    "        df = df.drop_duplicates(subset = 'quoteID', keep='first')\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.to_json(f\"./Quotebank/Sample_{year}_wrangled.json.bz2\",compression=\"bz2\",lines=True,orient=\"records\")\n",
    "        print(f\"Sanitiy check : len of df = {len(df)}\")\n",
    "        print(f\"Done with year {year}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTout est bien qui finit bien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c119a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small adjustments to default style of plots, making sure it's readable and colorblind-friendly everywhere\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size' : 12.5,\n",
    "                     'figure.figsize':(10,7)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40567d-0e3b-4f25-adfb-3e3cfa3734d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "year = 2015\n",
    "df = pd.read_json(f'./Quotebank/Sample_{year}_wrangled.json.bz2', lines=True, compression='bz2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb991-2fae-42f8-a66f-1370b28a1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot to analyze the presence of outliers\n",
    "data_plot = df.drop(['numOccurrences','year'], axis = 1)\n",
    "ax = sns.boxplot(data = data_plot)\n",
    "ax.set(ylabel='probabilities [-]')\n",
    "ax.set_title(f'Distribution of the probability of the speaker attribution {year}')\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.savefig(f\"{year}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70242ae4-8c1e-48bf-ab66-4ad139fa22a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
